{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f093836e-d3a6-4ee8-84f6-42d74d2939ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced from https://qiita.com/takubb/items/7d45ae701390912c7629\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# from tqdm import tqdm  #コマンドラインで実行するとき\n",
    "from tqdm.notebook import tqdm  # jupyter で実行するとき\n",
    "from models import RESNETLIKE, MyModel, MyModel_shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72020d7e-7827-401f-b3f4-b9ab250a9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocusPatchDataset(torch.utils.data.Dataset):\n",
    "    # FILEはデータが入っていたファイル、imsize = 画像サイズ、psize = パッチサイズ\n",
    "    def __init__(self, DIR, imsize, psize, channels, DEPTH_GAP, n_sample, transforms):\n",
    "        # ground truth depthを読み込むdd\n",
    "        with open(os.path.join(DIR,'depth.pkl'),\"rb\") as f:\n",
    "            self.gt = pickle.load(f)\n",
    "        \n",
    "        if self.gt is None:\n",
    "            print('cannot find ', os.path.join(DIR,'depth.pkl'))\n",
    "            return None\n",
    "        \n",
    "        self.files = glob.glob(os.path.join(DIR,'[0-9]*.bmp'))\n",
    "        # print(self.files)\n",
    "        self.fvalues = []\n",
    "        for fn in self.files:\n",
    "            self.fvalues.append(int(os.path.splitext(os.path.basename(fn))[0]))\n",
    "        self.fvalues.sort()\n",
    "        \n",
    "        self.DIR = DIR\n",
    "        self.DEPTH_GAP = DEPTH_GAP\n",
    "        self.psize = psize\n",
    "        self.channels = channels\n",
    "        self.transforms = transforms\n",
    "        self.n_sample = n_sample\n",
    "\n",
    "        self.imsize = imsize\n",
    "        self.psize = psize\n",
    "        self.locs = []\n",
    "\n",
    "        # サンプル対象となる点を設定する\n",
    "        w = imsize[0]\n",
    "        h = imsize[1]        \n",
    "        for i in range(self.n_sample):\n",
    "            xx = random.randint(0,w-self.psize-1)\n",
    "            yy = random.randint(0,h-self.psize-1)\n",
    "            self.locs.append([xx,yy])\n",
    "\n",
    "        print('locations', self.locs)\n",
    "\n",
    "        # 学習高速化のためバッファを用意する\n",
    "        self.buffer = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = (len(self.fvalues) - self.DEPTH_GAP)*self.n_sample\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgs = []\n",
    "\n",
    "        # n: file (data) index, m: point index\n",
    "        n = index // self.n_sample\n",
    "        m = index % self.n_sample\n",
    "\n",
    "        x = self.locs[m][0]\n",
    "        y = self.locs[m][1]\n",
    "        \n",
    "        val = float(self.gt[y,x]) - float(self.fvalues[n])\n",
    "\n",
    "        # もしバッファにデータがあるならそれを返す\n",
    "        if index in self.buffer.keys():\n",
    "            return self.transforms(self.buffer[index]), val\n",
    "\n",
    "        # バッファにない場合はファイルから読み出す\n",
    "        FILE1 = os.path.join(self.DIR,f'{self.fvalues[n]:04d}.bmp')\n",
    "        FILE2 = os.path.join(self.DIR,f'{self.fvalues[n+self.DEPTH_GAP]:04d}.bmp')\n",
    "        \n",
    "        img = Image.open(FILE1)\n",
    "        img = img.crop((x, y, x+self.psize, y+self.psize))\n",
    "        img = img.convert('L')\n",
    "        img = np.array(img)\n",
    "        img = np.array(img).astype('float32')\n",
    "        imgs.append(img)\n",
    "\n",
    "        img = Image.open(FILE2)\n",
    "        img = img.crop((x, y, x+self.psize, y+self.psize))\n",
    "        img = img.convert('L')\n",
    "        img = np.array(img)\n",
    "        img = np.array(img).astype('float32')\n",
    "        imgs.append(img)\n",
    "        \n",
    "        out = np.stack(imgs,axis=2)\n",
    "\n",
    "        # バッファーに保存する\n",
    "        self.buffer[index] = out\n",
    "        \n",
    "        return self.transforms(out), val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a7447-cbc4-4caa-a9ef-51c995fc2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル訓練関数\n",
    "def train_model(model, train_loader, test_loader):\n",
    "    # Train loop ----------------------------\n",
    "    model.train()  # 学習モードをオン\n",
    "    train_batch_loss = []\n",
    "    for data, val in train_loader:\n",
    "        # GPUへの転送\n",
    "        data, val = data.to(device), val.to(device)\n",
    "        # 1. 勾配リセット\n",
    "        optimizer.zero_grad()\n",
    "        # 2. 推論\n",
    "        output = model(data)\n",
    "        val = val.unsqueeze(1).to(torch.float32)\n",
    "        # 3. 誤差計算\n",
    "        loss = criterion(output, val)\n",
    "        # 4. 誤差逆伝播\n",
    "        loss.backward()\n",
    "        # 5. パラメータ更新\n",
    "        optimizer.step()\n",
    "        # train_lossの取得\n",
    "        train_batch_loss.append(loss.item())\n",
    "\n",
    "    # Test(val) loop ----------------------------\n",
    "    model.eval()  # 学習モードをオフ\n",
    "    test_batch_loss = []\n",
    "    with torch.no_grad():  # 勾配を計算なし\n",
    "        for data, val in test_loader:\n",
    "            data, val = data.to(device), val.to(device)\n",
    "            output = model(data)\n",
    "            val = val.unsqueeze(1).to(torch.float32)\n",
    "            loss = criterion(output, val)\n",
    "            test_batch_loss.append(loss.item())\n",
    "\n",
    "    return model, np.mean(train_batch_loss), np.mean(test_batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0502b77-4642-461a-9018-8e1aca510e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locations [[1285, 152], [502, 450], [994, 453], [118, 590], [1345, 117], [358, 124], [84, 882], [474, 172], [634, 165], [333, 42], [487, 752], [475, 752], [313, 366], [253, 328], [1412, 590], [1381, 510], [1390, 77], [1390, 24], [942, 959], [761, 301]]\n",
      "locations [[290, 535], [736, 447], [673, 660], [888, 570], [888, 1064], [1423, 921], [1058, 811], [1190, 1066], [96, 97], [605, 489], [0, 485], [1005, 677], [271, 201], [612, 123], [560, 496], [72, 237], [109, 473], [1384, 778], [4, 312], [94, 96]]\n",
      "locations [[1230, 264], [1201, 847], [868, 784], [474, 134], [1287, 990], [1418, 339], [829, 146], [667, 983], [288, 307], [508, 549], [1305, 1082], [348, 441], [1132, 41], [1244, 505], [89, 645], [1471, 281], [1387, 333], [256, 51], [890, 666], [208, 17]]\n",
      "locations [[727, 860], [626, 1009], [341, 941], [900, 1113], [961, 735], [1416, 432], [590, 703], [675, 207], [773, 1121], [70, 15], [95, 49], [615, 792], [1242, 1110], [1000, 948], [171, 738], [654, 857], [696, 135], [1444, 133], [1104, 544], [734, 1056]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # リソースの指定（CPU/GPU）\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # データセットの作成\n",
    "    trans = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # パラメータの設定\n",
    "    PSIZE = 65\n",
    "    DEPTH_GAP = 100\n",
    "    N_SAMPLE = 20\n",
    "    \n",
    "    # merge several dataset together\n",
    "    dataset1 = FocusPatchDataset(DIR = 'data/202410041703', imsize = [1600,1200], \n",
    "                                psize=PSIZE, channels=2, n_sample=N_SAMPLE, \n",
    "                                DEPTH_GAP=DEPTH_GAP, transforms = trans)\n",
    "    dataset2 = FocusPatchDataset(DIR = 'data/202410041747', imsize = [1600,1200], \n",
    "                                psize=PSIZE, channels=2, n_sample=N_SAMPLE, \n",
    "                                DEPTH_GAP=DEPTH_GAP, transforms = trans)\n",
    "    dataset3 = FocusPatchDataset(DIR = 'data/202410080808', imsize = [1600,1200], \n",
    "                                psize=PSIZE, channels=2, n_sample=N_SAMPLE, \n",
    "                                DEPTH_GAP=DEPTH_GAP, transforms = trans)\n",
    "    dataset4= FocusPatchDataset(DIR = 'data/202410080819', imsize = [1600,1200], \n",
    "                                psize=PSIZE, channels=2, n_sample=N_SAMPLE, \n",
    "                                DEPTH_GAP=DEPTH_GAP, transforms = trans)\n",
    "    \n",
    "    dataset = torch.utils.data.ConcatDataset([dataset1,dataset2,dataset3,dataset4])\n",
    "\n",
    "    # test dataset, train datasetに分割する\n",
    "    train_set, test_set = torch.utils.data.random_split(dataset, [0.9,0.1])\n",
    "    print(\"train dataset\", len(train_set), \"test dataset\", len(test_set))\n",
    "\n",
    "    # データローダーの作成\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                               batch_size=1024,  # バッチサイズ\n",
    "                                               shuffle=True,  # データシャッフル\n",
    "                                               num_workers=2,  # 高速化\n",
    "                                               pin_memory=True  # 高速化                                      \n",
    "                                               )\n",
    "    test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                              batch_size=1024,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=2,  # 高速化\n",
    "                                              pin_memory=True  # 高速化                                                   \n",
    "                                              )\n",
    "\n",
    "    # モデル・損失関数・最適化アルゴリスムの設定\n",
    "    LOAD_WEIGHT = True\n",
    "    # model = RESNETLIKE(channels=2).to(device)\n",
    "    model = MyModel(channels=2)\n",
    "    #model = MyModel_shallow(channels=2).to(device)\n",
    "    weight_file = f\"weights/weight_{PSIZE}_{DEPTH_GAP}.pth\"\n",
    "    \n",
    "    if LOAD_WEIGHT == True:\n",
    "        # 学習済みモデルのロード\n",
    "        print(\"load weight\", weight_file)\n",
    "        model.load_state_dict(torch.load(weight_file))\n",
    "        model.eval()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    #optimizer = optim.SGD(model.parameters())\n",
    "\n",
    "    # 訓練の実行\n",
    "    epoch = 30\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in tqdm(range(epoch)):\n",
    "        model, train_l, test_l = train_model(model,train_loader,test_loader)\n",
    "        train_loss.append(train_l)\n",
    "        test_loss.append(test_l)    \n",
    "        # 10エポックごとにロスを表示\n",
    "        #if epoch % 5 == 0:\n",
    "        print(f\"{epoch}: train loss: {train_loss[-1]:.3f}, test loss: {test_loss[-1]:.3f}\")\n",
    "    \n",
    "    # モデルの保存\n",
    "    print(\"save weight as: \", weight_file)\n",
    "    torch.save(model.state_dict(), weight_file)\n",
    "\n",
    "    # 学習状況（ロス）の確認\n",
    "    plt.plot(train_loss, label='train_loss')\n",
    "    plt.plot(test_loss, label='test_loss')\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
